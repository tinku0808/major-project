{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import shutil \n",
    "from pathlib import Path\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXTRACTED DATA FROM SEEDED FILE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV files have been copied to c:\\Users\\SabareeshwaranM\\Desktop\\FINAL_PROJECT\\DE_MAJOR_PROJECT-main\\raw_table\n"
     ]
    }
   ],
   "source": [
    "# Define paths\n",
    "downloads_path = str(Path.home() / \"Documents\" / \"major_project\")  # Path to the source directory\n",
    "notebook_dir = os.getcwd()  # Current working directory\n",
    "raw_table = os.path.join(notebook_dir, \"raw_table\")  # Destination directory\n",
    "\n",
    "# Ensure the destination directory exists\n",
    "os.makedirs(raw_table, exist_ok=True)\n",
    "\n",
    "# List files in the source directory\n",
    "files = os.listdir(downloads_path)\n",
    "\n",
    "# Copy files starting with 'RAW' and are CSV\n",
    "for file in files:\n",
    "    if file.startswith(\"RAW\") and file.endswith(\".csv\"):  # Ensure it's a CSV file\n",
    "        shutil.copy(os.path.join(downloads_path, file), raw_table)\n",
    "\n",
    "print(f\"CSV files have been copied to {raw_table}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RAW TABLES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths for the CSV files\n",
    "feedbacks_path = './raw_table/RAW.feedbacks.csv'\n",
    "learningmaterial_path = './raw_table/RAW.learningmaterials.csv'\n",
    "quizzes_path = './raw_table/RAW.quizzes.csv'\n",
    "scores_path = './raw_table/RAW.scores.csv'\n",
    "users_path = './raw_table/RAW.users.csv'\n",
    "\n",
    "# Reading CSV files into DataFrames\n",
    "feedbacks_df = pd.read_csv(feedbacks_path)\n",
    "learningmaterial_df = pd.read_csv(learningmaterial_path)\n",
    "quizzes_df = pd.read_csv(quizzes_path)\n",
    "scores_df = pd.read_csv(scores_path)\n",
    "users_df = pd.read_csv(users_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CHECKING AND CLEANING NULL VALUES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null values in feedbacks.csv:\n",
      "_id                      0\n",
      "employeeId               0\n",
      "learningMaterialTitle    0\n",
      "description              0\n",
      "rating                   0\n",
      "createdAt                0\n",
      "__v                      0\n",
      "dtype: int64\n",
      "\n",
      "Null values in learningmaterial.csv:\n",
      "_id            0\n",
      "title          0\n",
      "description    0\n",
      "createdBy      0\n",
      "createdAt      0\n",
      "__v            0\n",
      "dtype: int64\n",
      "\n",
      "Null values in quizzes.csv:\n",
      "_id                                   0\n",
      "title                                 0\n",
      "questions[0].questionText             0\n",
      "questions[0].options[0].optionText    0\n",
      "questions[0].options[1].optionText    0\n",
      "questions[0].options[2].optionText    0\n",
      "questions[0].options[0].isCorrect     0\n",
      "questions[0].options[1].isCorrect     0\n",
      "questions[0].options[2].isCorrect     0\n",
      "questions[0].options[0]._id           0\n",
      "questions[0].options[1]._id           0\n",
      "questions[0].options[2]._id           0\n",
      "questions[0]._id                      0\n",
      "learningMaterial                      0\n",
      "createdAt                             0\n",
      "__v                                   0\n",
      "dtype: int64\n",
      "\n",
      "Null values in scores.csv:\n",
      "_id           0\n",
      "employeeId    0\n",
      "quizId        0\n",
      "score         0\n",
      "timeSpent     0\n",
      "completed     0\n",
      "__v           0\n",
      "dtype: int64\n",
      "\n",
      "Null values in users.csv:\n",
      "_id           0\n",
      "name          0\n",
      "email         0\n",
      "password      0\n",
      "role          0\n",
      "team          0\n",
      "department    0\n",
      "employeeId    0\n",
      "__v           0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Checking for null values in each DataFrame\n",
    "print(\"Null values in feedbacks.csv:\")\n",
    "print(feedbacks_df.isnull().sum())\n",
    "print(\"\\nNull values in learningmaterial.csv:\")\n",
    "print(learningmaterial_df.isnull().sum())\n",
    "print(\"\\nNull values in quizzes.csv:\")\n",
    "print(quizzes_df.isnull().sum())\n",
    "print(\"\\nNull values in scores.csv:\")\n",
    "print(scores_df.isnull().sum())\n",
    "print(\"\\nNull values in users.csv:\")\n",
    "print(users_df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TRANSFORMING THE TABLE AND SAVING AS PREP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# users_df['email'] = users_df['email'].str.lower()   # Normalize email to lowercase\n",
    "\n",
    "\n",
    "feedbacks_df['createdAt'] = pd.to_datetime(feedbacks_df['createdAt'])  # Convert to datetime\n",
    "\n",
    "\n",
    "learningmaterial_df['createdAt'] = pd.to_datetime(learningmaterial_df['createdAt'])\n",
    "\n",
    "\n",
    "quizzes_df['createdAt'] = pd.to_datetime(quizzes_df['createdAt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Droping Unwanted Coloumns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feedbacks_df.dtypes\n",
    "\n",
    "feedbacks_df.drop(columns=['description', '__v'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "users_df.drop(columns=['__v'], inplace=True)\n",
    "\n",
    "learningmaterial_df.drop(columns=['description', '__v'], inplace=True)\n",
    "\n",
    "selected_columns = ['_id', 'title', 'learningMaterial','createdAt']\n",
    "quizzes_df = quizzes_df.loc[:, selected_columns]\n",
    "\n",
    "scores_df.drop(columns=['__v'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrames successfully saved in './prep_table'\n"
     ]
    }
   ],
   "source": [
    "output_folder = './prep_table'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# File paths for the new CSVs\n",
    "users_output_path = os.path.join(output_folder, 'PREP.users.csv')\n",
    "learningmaterial_output_path = os.path.join(output_folder, 'PREP.learningmaterials.csv')\n",
    "quizzes_output_path = os.path.join(output_folder, 'PREP.quizzes.csv')\n",
    "scores_output_path = os.path.join(output_folder, 'PREP.scores.csv')\n",
    "feedbacks_output_path = os.path.join(output_folder, 'PREP.feedbacks.csv')\n",
    "\n",
    "# Save the DataFrames to CSV files in the new folder\n",
    "users_df.to_csv(users_output_path, index=False)\n",
    "learningmaterial_df.to_csv(learningmaterial_output_path, index=False)\n",
    "quizzes_df.to_csv(quizzes_output_path, index=False)\n",
    "scores_df.to_csv(scores_output_path, index=False)\n",
    "feedbacks_df.to_csv(feedbacks_output_path, index=False)\n",
    "\n",
    "print(f\"DataFrames successfully saved in '{output_folder}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FACT AND DIMENSIONAL TABLES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema of prep_engagements:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 6 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   _id         1000 non-null   object\n",
      " 1   employeeId  1000 non-null   int64 \n",
      " 2   quizId      1000 non-null   object\n",
      " 3   score       1000 non-null   int64 \n",
      " 4   timeSpent   1000 non-null   int64 \n",
      " 5   completed   1000 non-null   bool  \n",
      "dtypes: bool(1), int64(3), object(2)\n",
      "memory usage: 40.2+ KB\n",
      "None\n",
      "\n",
      "\n",
      "Schema of prep_quizzes:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 4 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   _id               1000 non-null   object\n",
      " 1   title             1000 non-null   object\n",
      " 2   learningMaterial  1000 non-null   object\n",
      " 3   createdAt         1000 non-null   object\n",
      "dtypes: object(4)\n",
      "memory usage: 31.4+ KB\n",
      "None\n",
      "\n",
      "\n",
      "Schema of prep_feedbacks:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 5 columns):\n",
      " #   Column                 Non-Null Count  Dtype \n",
      "---  ------                 --------------  ----- \n",
      " 0   _id                    1000 non-null   object\n",
      " 1   employeeId             1000 non-null   int64 \n",
      " 2   learningMaterialTitle  1000 non-null   object\n",
      " 3   rating                 1000 non-null   int64 \n",
      " 4   createdAt              1000 non-null   object\n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 39.2+ KB\n",
      "None\n",
      "\n",
      "\n",
      "Schema of prep_learningmaterials:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   _id        1000 non-null   object\n",
      " 1   title      1000 non-null   object\n",
      " 2   createdBy  1000 non-null   object\n",
      " 3   createdAt  1000 non-null   object\n",
      "dtypes: object(4)\n",
      "memory usage: 31.4+ KB\n",
      "None\n",
      "\n",
      "\n",
      "Schema of prep_users:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 250 entries, 0 to 249\n",
      "Data columns (total 8 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   _id         250 non-null    object\n",
      " 1   name        250 non-null    object\n",
      " 2   email       250 non-null    object\n",
      " 3   password    250 non-null    object\n",
      " 4   role        250 non-null    object\n",
      " 5   team        250 non-null    object\n",
      " 6   department  250 non-null    object\n",
      " 7   employeeId  250 non-null    int64 \n",
      "dtypes: int64(1), object(7)\n",
      "memory usage: 15.8+ KB\n",
      "None\n",
      "\n",
      "\n",
      "Dimensional and fact tables have been saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "df_selected_scores = pd.read_csv(\"./prep_table/PREP.scores.csv\")\n",
    "df_selected_quizzes = pd.read_csv(\"./prep_table/PREP.quizzes.csv\")\n",
    "df_feedbacks = pd.read_csv(\"./prep_table/PREP.feedbacks.csv\")\n",
    "df_learningmaterial = pd.read_csv(\"./prep_table/PREP.learningmaterials.csv\")\n",
    "df_users = pd.read_csv(\"./prep_table/PREP.users.csv\")\n",
    "\n",
    "# Define a function to print the schema of a DataFrame\n",
    "def print_schema(df, name):\n",
    "    print(f\"Schema of {name}:\")\n",
    "    print(df.info())\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Print the schema for each DataFrame\n",
    "print_schema(df_selected_scores, \"prep_engagements\")\n",
    "print_schema(df_selected_quizzes, \"prep_quizzes\")\n",
    "print_schema(df_feedbacks, \"prep_feedbacks\")\n",
    "print_schema(df_learningmaterial, \"prep_learningmaterials\")\n",
    "print_schema(df_users, \"prep_users\")\n",
    "\n",
    "# Rename _id to quizId in the quizzes table to ensure consistency\n",
    "df_selected_quizzes.rename(columns={'_id': 'quizId'}, inplace=True)\n",
    "\n",
    "# Create dimension tables\n",
    "dim_employees = df_users[['employeeId', 'name', 'email', 'role', 'team', 'department']].drop_duplicates()\n",
    "dim_quizzes = df_selected_quizzes[['quizId', 'title', 'learningMaterial', 'createdAt']].drop_duplicates()\n",
    "dim_learning_materials = df_learningmaterial[['title', 'createdBy', 'createdAt']].drop_duplicates()\n",
    "\n",
    "# Create fact tables\n",
    "fact_engagements = df_selected_scores[['employeeId', 'quizId', 'score', 'timeSpent', 'completed']].copy()\n",
    "fact_feedback = df_feedbacks[['employeeId', 'learningMaterialTitle', 'rating', 'createdAt']].copy()\n",
    "\n",
    "# Define the prep_table folder path\n",
    "prep_table_folder = \"./prep_table\"\n",
    "\n",
    "\n",
    "dim_employees.to_csv(os.path.join(prep_table_folder, 'DIM.employees.csv'), index=False)\n",
    "dim_quizzes.to_csv(os.path.join(prep_table_folder, 'DIM.quizzes.csv'), index=False)\n",
    "dim_learning_materials.to_csv(os.path.join(prep_table_folder, 'DIM.learning_materials.csv'), index=False)\n",
    "\n",
    "\n",
    "fact_engagements.to_csv(os.path.join(prep_table_folder, 'FACT.engagements.csv'), index=False)\n",
    "fact_feedback.to_csv(os.path.join(prep_table_folder, 'FACT.feedback.csv'), index=False)\n",
    "\n",
    "print(\"Dimensional and fact tables have been saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**REPORTING LAYER**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feedback Summary Report**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>employeeId</th>\n",
       "      <th>learningMaterialTitle</th>\n",
       "      <th>rating</th>\n",
       "      <th>createdAt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>106</td>\n",
       "      <td>Despecto necessitatibus aurum summisse labore ...</td>\n",
       "      <td>4</td>\n",
       "      <td>2024-10-07 09:53:23.030000+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24</td>\n",
       "      <td>Tener odio cubicularis argentum aperio.</td>\n",
       "      <td>5</td>\n",
       "      <td>2024-10-07 09:53:23.058000+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>115</td>\n",
       "      <td>Cruentus sol spectaculum.</td>\n",
       "      <td>5</td>\n",
       "      <td>2024-10-07 09:53:23.081000+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>32</td>\n",
       "      <td>Adipiscor sunt crebro acsi decens quam excepturi.</td>\n",
       "      <td>3</td>\n",
       "      <td>2024-10-07 09:53:23.105000+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>203</td>\n",
       "      <td>Anser urbanus decet ter ubi vomito decretum vo...</td>\n",
       "      <td>3</td>\n",
       "      <td>2024-10-07 09:53:23.128000+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>217</td>\n",
       "      <td>Tego cupiditate demens arcesso supplanto.</td>\n",
       "      <td>1</td>\n",
       "      <td>2024-10-07 09:53:46.759000+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>17</td>\n",
       "      <td>Curso ultra crinis tripudio celo sui tenetur a...</td>\n",
       "      <td>2</td>\n",
       "      <td>2024-10-07 09:53:46.782000+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>222</td>\n",
       "      <td>Venio tunc ventosus sunt tabgo ocer demo reici...</td>\n",
       "      <td>2</td>\n",
       "      <td>2024-10-07 09:53:46.806000+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>182</td>\n",
       "      <td>Asper supplanto auditor corporis deputo civita...</td>\n",
       "      <td>4</td>\n",
       "      <td>2024-10-07 09:53:46.831000+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>70</td>\n",
       "      <td>Tametsi harum conventus temperantia autus cari...</td>\n",
       "      <td>5</td>\n",
       "      <td>2024-10-07 09:53:46.866000+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     employeeId                              learningMaterialTitle  rating  \\\n",
       "0           106  Despecto necessitatibus aurum summisse labore ...       4   \n",
       "1            24            Tener odio cubicularis argentum aperio.       5   \n",
       "2           115                          Cruentus sol spectaculum.       5   \n",
       "3            32  Adipiscor sunt crebro acsi decens quam excepturi.       3   \n",
       "4           203  Anser urbanus decet ter ubi vomito decretum vo...       3   \n",
       "..          ...                                                ...     ...   \n",
       "995         217          Tego cupiditate demens arcesso supplanto.       1   \n",
       "996          17  Curso ultra crinis tripudio celo sui tenetur a...       2   \n",
       "997         222  Venio tunc ventosus sunt tabgo ocer demo reici...       2   \n",
       "998         182  Asper supplanto auditor corporis deputo civita...       4   \n",
       "999          70  Tametsi harum conventus temperantia autus cari...       5   \n",
       "\n",
       "                            createdAt  \n",
       "0    2024-10-07 09:53:23.030000+00:00  \n",
       "1    2024-10-07 09:53:23.058000+00:00  \n",
       "2    2024-10-07 09:53:23.081000+00:00  \n",
       "3    2024-10-07 09:53:23.105000+00:00  \n",
       "4    2024-10-07 09:53:23.128000+00:00  \n",
       "..                                ...  \n",
       "995  2024-10-07 09:53:46.759000+00:00  \n",
       "996  2024-10-07 09:53:46.782000+00:00  \n",
       "997  2024-10-07 09:53:46.806000+00:00  \n",
       "998  2024-10-07 09:53:46.831000+00:00  \n",
       "999  2024-10-07 09:53:46.866000+00:00  \n",
       "\n",
       "[1000 rows x 4 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fact_feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "reporting_folder = \"./report_table\"\n",
    "\n",
    "feedback_summary = fact_feedback.groupby('learningMaterialTitle').agg({'rating': 'mean', 'employeeId': 'count'}).reset_index()\n",
    "feedback_summary.columns = ['learningMaterialTitle', 'average_rating', 'feedback_count']\n",
    "feedback_summary.to_csv(os.path.join(reporting_folder, 'feedback_summary_report.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**User Feedback Summary Report**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User Feedback Summary\n",
    "user_feedback_summary = fact_feedback.groupby('employeeId').agg({\n",
    "    'rating': ['mean', 'count']\n",
    "}).reset_index()\n",
    "\n",
    "# Flatten the multi-level columns\n",
    "user_feedback_summary.columns = ['employeeId', 'average_rating', 'total_feedback_count']\n",
    "\n",
    "# Merge with employee dimension to get user names\n",
    "user_feedback_summary = user_feedback_summary.merge(dim_employees[['employeeId', 'name']], on='employeeId', how='left')\n",
    "\n",
    "# Reorder columns for better readability\n",
    "user_feedback_summary = user_feedback_summary[['employeeId', 'name', 'total_feedback_count', 'average_rating']]\n",
    "user_feedback_summary.to_csv(os.path.join(reporting_folder, 'user_feedback_summary_report.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**User Engagement Report**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User Engagement Report\n",
    "user_engagement_report = fact_engagements.groupby('employeeId').agg({\n",
    "    'score': ['sum', 'count', 'mean'],  # Total score, total quizzes taken, average score\n",
    "    'timeSpent': 'sum'                   # Total time spent\n",
    "}).reset_index()\n",
    "\n",
    "# Flatten the multi-level columns\n",
    "user_engagement_report.columns = ['employeeId', 'total_score', 'total_quizzes_taken', 'average_score', 'total_time_spent']\n",
    "\n",
    "# Merge with employee dimension to get user names and department\n",
    "user_engagement_report = user_engagement_report.merge(\n",
    "    dim_employees[['employeeId', 'name', 'department']], \n",
    "    on='employeeId', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Reorder columns for better readability\n",
    "user_engagement_report = user_engagement_report[['employeeId', 'name', 'department', 'total_quizzes_taken', 'total_score', 'average_score', 'total_time_spent']]\n",
    "\n",
    "\n",
    "user_engagement_report.to_csv(os.path.join(reporting_folder, 'user_engagement_report.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating Engagement Score By Calculating Some Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     employeeId         name_feedback  total_feedback_count  average_rating  \\\n",
      "0            11        Mr. Van Howell                    10        1.800000   \n",
      "1            12  Dr. Emmett Gleichner                     3        2.666667   \n",
      "2            13  Dr. Alfredo Mosciski                     7        3.714286   \n",
      "3            14           Lula Harris                     5        3.200000   \n",
      "4            15     Ricardo Dicki DVM                     5        2.600000   \n",
      "..          ...                   ...                   ...             ...   \n",
      "228         246          Essie Mayert                     4        2.500000   \n",
      "229         247     Marlene Jaskolski                     1        5.000000   \n",
      "230         248           Bryan Fahey                     2        3.000000   \n",
      "231         249          May Nitzsche                     3        3.333333   \n",
      "232         250            Todd Rohan                     1        3.000000   \n",
      "\n",
      "          name_engagement   department  total_quizzes_taken  total_score  \\\n",
      "0          Mr. Van Howell  Electronics                    5           85   \n",
      "1    Dr. Emmett Gleichner     Clothing                    2          106   \n",
      "2    Dr. Alfredo Mosciski      Jewelry                    3           26   \n",
      "3             Lula Harris       Sports                    6          265   \n",
      "4       Ricardo Dicki DVM         Baby                    5          165   \n",
      "..                    ...          ...                  ...          ...   \n",
      "228          Essie Mayert        Books                    7          457   \n",
      "229     Marlene Jaskolski         Home                    7          378   \n",
      "230           Bryan Fahey         Toys                    6          217   \n",
      "231          May Nitzsche       Garden                    6          299   \n",
      "232            Todd Rohan       Sports                    4          149   \n",
      "\n",
      "     average_score  total_time_spent  engagement_score  \n",
      "0        17.000000               365         46.964856  \n",
      "1        53.000000                99         15.131345  \n",
      "2         8.666667                90          8.836655  \n",
      "3        44.166667               451         61.683529  \n",
      "4        33.000000               270         35.889244  \n",
      "..             ...               ...               ...  \n",
      "228      65.285714               250         37.644531  \n",
      "229      54.000000               433         60.436635  \n",
      "230      36.166667               308         41.156372  \n",
      "231      49.833333               336         46.880547  \n",
      "232      37.250000               299         39.703142  \n",
      "\n",
      "[233 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "# Read the CSV files\n",
    "df_user_feedback_summary = pd.read_csv('./report_table/user_feedback_summary_report.csv')\n",
    "df_engagement_report = pd.read_csv('./report_table/user_engagement_report.csv')\n",
    "\n",
    "# Merge user_feedback_summary with user_engagement_report on employeeId\n",
    "merged_df = pd.merge(\n",
    "    df_user_feedback_summary,\n",
    "    df_engagement_report,\n",
    "    on='employeeId',\n",
    "    suffixes=('_feedback', '_engagement')\n",
    ")\n",
    "\n",
    "# Calculate engagement score\n",
    "merged_df['engagement_score'] = (\n",
    "    0.2 * merged_df['average_rating'] +\n",
    "    0.2 * merged_df['total_quizzes_taken'] +\n",
    "    0.2 * merged_df['average_score'] +\n",
    "    0.2 * merged_df['total_time_spent'] +\n",
    "    0.2 * merged_df['total_feedback_count']\n",
    ")\n",
    "\n",
    "# Normalize the engagement score and scale by 100\n",
    "merged_df['engagement_score'] = (\n",
    "    (merged_df['engagement_score'] - merged_df['engagement_score'].min()) / \n",
    "    (merged_df['engagement_score'].max() - merged_df['engagement_score'].min())\n",
    ") * 100\n",
    "\n",
    "print(merged_df)\n",
    "\n",
    "\n",
    "reporting_folder = './report_table'  # Define your reporting folder path\n",
    "merged_df.to_csv(os.path.join(reporting_folder, 'final_reporting_table.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema of report_table:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2000 entries, 0 to 1999\n",
      "Data columns (total 18 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   employeeId             2000 non-null   int64  \n",
      " 1   quizId                 1000 non-null   object \n",
      " 2   score                  1000 non-null   float64\n",
      " 3   timeSpent              1000 non-null   float64\n",
      " 4   completed              1000 non-null   object \n",
      " 5   name                   2000 non-null   object \n",
      " 6   email                  2000 non-null   object \n",
      " 7   role                   2000 non-null   object \n",
      " 8   team                   2000 non-null   object \n",
      " 9   department             2000 non-null   object \n",
      " 10  title                  2000 non-null   object \n",
      " 11  learningMaterial       1000 non-null   object \n",
      " 12  createdAt              1000 non-null   object \n",
      " 13  learningMaterialTitle  1000 non-null   object \n",
      " 14  rating                 1000 non-null   float64\n",
      " 15  createdAt_x            1000 non-null   object \n",
      " 16  createdBy              1000 non-null   object \n",
      " 17  createdAt_y            1000 non-null   object \n",
      "dtypes: float64(3), int64(1), object(14)\n",
      "memory usage: 281.4+ KB\n",
      "None\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Merge data to create the report table\n",
    "# First, join engagements with employee and quiz details\n",
    "report_engagements = fact_engagements.merge(dim_employees, on='employeeId', how='left').merge(dim_quizzes, on='quizId', how='left')\n",
    "\n",
    "# Then, join feedback with employee and learning material details\n",
    "report_feedback = fact_feedback.merge(dim_employees, on='employeeId', how='left').merge(dim_learning_materials, left_on='learningMaterialTitle', right_on='title', how='left')\n",
    "\n",
    "# Merge engagements and feedbacks into a final report\n",
    "report_table = pd.concat([report_engagements, report_feedback], ignore_index=True)\n",
    "\n",
    "\n",
    "print_schema(report_table, \"report_table\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Top 10 employees based on score**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Employees Based on Score:\n",
      "                name      score\n",
      "0     Dr. Joel Block  97.000000\n",
      "1  Irving Deckow Jr.  89.333333\n",
      "2  Mr. Troy Medhurst  88.000000\n",
      "3  Jacqueline Cronin  84.000000\n",
      "4       Lucia Wisozk  81.500000\n",
      "5         Mack Wolff  81.000000\n",
      "6  Santiago Kassulke  80.000000\n",
      "7     Stanley Herman  80.000000\n",
      "8  Roberto Schroeder  79.000000\n",
      "9        Paul Wehner  78.500000\n"
     ]
    }
   ],
   "source": [
    "# Load the report_table.csv from the reporting folder\n",
    "reporting_folder = \"./report_table\"\n",
    "report_table = pd.read_csv(os.path.join(reporting_folder, 'reporting_table.csv'))\n",
    "\n",
    "# Group by employeeId and calculate the mean score\n",
    "top_10_scores = report_table.groupby('employeeId')['score'].mean().reset_index()\n",
    "\n",
    "# Sort by score in descending order and get the top 10 employees\n",
    "top_10_scores = top_10_scores.sort_values(by='score', ascending=False).head(10)\n",
    "\n",
    "# Merge with report_table to get employee names and other details\n",
    "top_10_scores = top_10_scores.merge(report_table[['employeeId', 'name']].drop_duplicates(), on='employeeId', how='left')\n",
    "\n",
    "\n",
    "print(\"Top 10 Employees Based on Score:\")\n",
    "print(top_10_scores[['name', 'score']])\n",
    "\n",
    "\n",
    "top_10_scores.to_csv(os.path.join(reporting_folder, 'top_10_employees_by_score.csv'), index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Top 10 employees based on time spent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 Employees Based on Time Spent:\n",
      "                    name   timeSpent\n",
      "0         Dr. Joel Block  117.000000\n",
      "1    Dr. Jennifer Sporer  117.000000\n",
      "2      Santiago Kassulke  113.000000\n",
      "3         Fred O'Connell  107.750000\n",
      "4        Billy Schroeder  106.333333\n",
      "5         Geraldine West  103.250000\n",
      "6         Cynthia Russel  102.166667\n",
      "7  Lynette Streich-Berge  101.000000\n",
      "8          Wanda Johnson  100.000000\n",
      "9          Lynette Thiel   99.500000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the report_table.csv from the reporting folder\n",
    "reporting_folder = \"./report_table\"\n",
    "report_table = pd.read_csv(os.path.join(reporting_folder, 'reporting_table.csv'))\n",
    "\n",
    "# Group by employeeId and calculate the mean timeSpent\n",
    "top_10_time_spent = report_table.groupby('employeeId')['timeSpent'].mean().reset_index()\n",
    "\n",
    "# Sort by timeSpent in descending order and get the top 10 employees\n",
    "top_10_time_spent = top_10_time_spent.sort_values(by='timeSpent', ascending=False).head(10)\n",
    "\n",
    "# Merge with report_table to get employee names and other details\n",
    "top_10_time_spent = top_10_time_spent.merge(report_table[['employeeId', 'name']].drop_duplicates(), on='employeeId', how='left')\n",
    "\n",
    "\n",
    "print(\"\\nTop 10 Employees Based on Time Spent:\")\n",
    "print(top_10_time_spent[['name', 'timeSpent']])\n",
    "\n",
    "top_10_time_spent.to_csv(os.path.join(reporting_folder, 'top_10_employees_by_time_spent.csv'), index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Top and least title based on feedback**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top Learning Material Title by Average Rating:\n",
      "                                 learningMaterialTitle  average_rating\n",
      "376  Quia adulescens validus deporto circumvenio ac...             5.0\n",
      "619  Vorago unde voveo tribuo creta complectus tondeo.             5.0\n",
      "372   Praesentium auctus addo solus adduco non stipes.             5.0\n",
      "625                            Vulgo subiungo tamquam.             5.0\n",
      "591  Vilitas cena tabesco benigne vobis tenus subit...             5.0\n",
      "582   Vetus auxilium absum tardus qui concedo allatus.             5.0\n",
      "357                        Pariatur temptatio dapifer.             5.0\n",
      "390  Reprehenderit testimonium synagoga clibanus de...             5.0\n",
      "19                                 Adhaero sto aufero.             5.0\n",
      "378  Quis demoror aut pecus natus sursum eveniet st...             5.0\n",
      "\n",
      "Least Learning Material Title by Average Rating:\n",
      "                                 learningMaterialTitle  average_rating\n",
      "627  Vulnus tandem assumenda solvo delego dolor acc...             1.0\n",
      "25   Adnuo deduco autus talus aperte corrigo delinq...             1.0\n",
      "588                         Vigilo vel aurum depopulo.             1.0\n",
      "43   Alias tabgo pecto desparatus caelum stipes aud...             1.0\n",
      "36                    Aeternus ultio aetas terminatio.             1.0\n",
      "529           Tripudio cattus appello defetiscor vito.             1.0\n",
      "70   Animi sui conduco autem eius viscus damnatio a...             1.0\n",
      "506  Theatrum deludo pariatur fugiat voluntarius ve...             1.0\n",
      "535  Tunc aestivus deduco templum absum cubitum lib...             1.0\n",
      "521  Totus tyrannus veritas cattus utor exercitatio...             1.0\n"
     ]
    }
   ],
   "source": [
    "# Load the report_table.csv from the reporting folder\n",
    "reporting_folder = \"./report_table\"\n",
    "report_table = pd.read_csv(os.path.join(reporting_folder, 'reporting_table.csv'))\n",
    "\n",
    "# Group feedback by 'learningMaterialTitle' and calculate the average rating for each title\n",
    "feedback_ratings = report_table.groupby('learningMaterialTitle')['rating'].mean().reset_index(name='average_rating')\n",
    "\n",
    "# Sort by average rating to find the top and least-rated learning material\n",
    "top_feedback = feedback_ratings.sort_values(by='average_rating', ascending=False).head(10)\n",
    "least_feedback = feedback_ratings.sort_values(by='average_rating', ascending=True).head(10)\n",
    "\n",
    "\n",
    "print(\"\\nTop Learning Material Title by Average Rating:\")\n",
    "print(top_feedback)\n",
    "\n",
    "print(\"\\nLeast Learning Material Title by Average Rating:\")\n",
    "print(least_feedback)\n",
    "\n",
    "\n",
    "top_feedback.to_csv(os.path.join(reporting_folder, 'top_10_Courses_by_feedback.csv'), index=False)\n",
    "least_feedback.to_csv(os.path.join(reporting_folder, 'least_10_Courses_by_feedback.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Titles of learning material with highest and lowest average score**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 Learning Material Titles by Average Score:\n",
      "                                                 title  score\n",
      "928                             Valeo arguo temeritas.  100.0\n",
      "861  Curatio sed solio utique voluptate adsidue the...  100.0\n",
      "680                        Alo commodo placeat vitium.  100.0\n",
      "61   Copiose utilis tricesimus turbo comis certus p...  100.0\n",
      "859        Curiositas vita aeneus cunabula ut ascisco.  100.0\n",
      "358  Vere aufero ad subseco alii aspernatur curto b...   99.0\n",
      "69   Utor similique una suppono adaugeo consuasor n...   99.0\n",
      "302                      Titulus tremo depraedor quod.   98.0\n",
      "745  Valde tum appositus clam admoveo rem depraedor...   98.0\n",
      "941  Uter tener libero arbustum temeritas ullam adf...   98.0\n",
      "\n",
      "Last 10 Learning Material Titles by Average Score:\n",
      "                                                 title  score\n",
      "671  Delectus pariatur basium earum velociter ambit...    1.0\n",
      "33   Templum inventore eum subvenio aranea clibanus...    1.0\n",
      "406                              Adeptio cribro capto.    1.0\n",
      "393      At claudeo ademptio speciosus vesica alioqui.    1.0\n",
      "875  Arbustum facilis templum vinum dicta creber ve...    0.0\n",
      "199  Nisi dolorum tametsi tamdiu corpus tempora val...    0.0\n",
      "357  Truculenter desino amplus tersus impedit xiphi...    0.0\n",
      "332                           Ascisco libero antiquus.    0.0\n",
      "331  Ciminatio quia id ut vel depraedor truculenter...    0.0\n",
      "422  Attollo arca decimus adsidue caelestis approbo...    0.0\n"
     ]
    }
   ],
   "source": [
    "# Group scores by 'quizId' and calculate the average score for each quiz\n",
    "average_scores = report_table.groupby('quizId')['score'].mean().reset_index()\n",
    "\n",
    "# Merge with report_table to get the titles associated with the quizzes\n",
    "average_scores = average_scores.merge(report_table[['title', 'quizId']], left_on='quizId', right_on='quizId', how='left')\n",
    "\n",
    "# Sort by average score in descending order\n",
    "average_scores_sorted = average_scores.sort_values(by='score', ascending=False)\n",
    "\n",
    "# Get top 10 titles with the highest average scores\n",
    "top_10_titles = average_scores_sorted.head(10)\n",
    "\n",
    "# Get last 10 titles with the lowest average scores\n",
    "last_10_titles = average_scores_sorted.tail(10)\n",
    "\n",
    "\n",
    "print(\"\\nTop 10 Learning Material Titles by Average Score:\")\n",
    "print(top_10_titles[['title', 'score']])\n",
    "\n",
    "print(\"\\nLast 10 Learning Material Titles by Average Score:\")\n",
    "print(last_10_titles[['title', 'score']])\n",
    "\n",
    "\n",
    "top_10_titles[['title', 'score']].to_csv(os.path.join(reporting_folder, 'Top_10_Courses_based_on_score.csv'), index=False)\n",
    "last_10_titles[['title', 'score']].to_csv(os.path.join(reporting_folder, 'Last_10_Courses_based_on_score.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Department performance based on average score**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 Best Performing Departments by Average Score:\n",
      "    department      score\n",
      "5    Computers  62.265176\n",
      "10      Health  56.209402\n",
      "3        Books  52.509554\n",
      "21        Toys  51.752556\n",
      "11        Home  50.580952\n",
      "14        Kids  49.460976\n",
      "16       Music  48.604430\n",
      "18       Shoes  48.010601\n",
      "4     Clothing  47.975976\n",
      "12  Industrial  47.712171\n",
      "\n",
      "Bottom 10 Least Performing Departments by Average Score:\n",
      "    department      score\n",
      "0   Automotive  46.970149\n",
      "8       Garden  46.761905\n",
      "9      Grocery  46.016835\n",
      "1         Baby  45.937669\n",
      "15      Movies  45.584746\n",
      "20       Tools  45.565909\n",
      "13     Jewelry  43.505464\n",
      "19      Sports  42.831120\n",
      "17    Outdoors  40.085791\n",
      "7        Games  31.631399\n"
     ]
    }
   ],
   "source": [
    "# Merge employee information with scores based on employeeId\n",
    "department_performance = report_table[['employeeId', 'department']].merge(report_table[['employeeId', 'score']], on='employeeId', how='left')\n",
    "\n",
    "# Calculate the average score for each department\n",
    "average_department_scores = department_performance.groupby('department')['score'].mean().reset_index()\n",
    "\n",
    "# Sort the departments by average score\n",
    "average_department_scores_sorted = average_department_scores.sort_values(by='score', ascending=False)\n",
    "\n",
    "# Get top 10 best performing departments\n",
    "top_10_departments = average_department_scores_sorted.head(10)\n",
    "\n",
    "# Get bottom 10 least performing departments\n",
    "bottom_10_departments = average_department_scores_sorted.tail(10)\n",
    "\n",
    "\n",
    "print(\"\\nTop 10 Best Performing Departments by Average Score:\")\n",
    "print(top_10_departments)\n",
    "\n",
    "print(\"\\nBottom 10 Least Performing Departments by Average Score:\")\n",
    "print(bottom_10_departments)\n",
    "\n",
    "\n",
    "top_10_departments[['department', 'score']].to_csv(os.path.join(reporting_folder, 'best_performing_department.csv'), index=False)\n",
    "bottom_10_departments[['department', 'score']].to_csv(os.path.join(reporting_folder, 'worst_performing_department.csv'), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
